== Configuring Ollama

.Serving Runtime
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/apiProtocol: REST
    opendatahub.io/hardware-profile-name: migrated-gpu-mglzi-serving
    opendatahub.io/template-display-name: Ollama
    opendatahub.io/template-name: ollama
    openshift.io/display-name: granite-3b-code-base-2k
  name: ollama-serving-runtime
  namespace: ai-project
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
    - env:
        - name: OLLAMA_MODELS
          value: /.ollama/models
        - name: OLLAMA_HOST
          value: 0.0.0.0
        - name: OLLAMA_KEEP_ALIVE
          value: '-1m'
        - name: OLLAMA_CONTEXT_LENGTH
          value: '32768'
        - name: OLLAMA_NUM_PARALLEL
          value: '1'
        - name: OLLAMA_FLASH_ATTENTION
          value: 'true'
        - name: OLLAMA_NEW_ENGINE
          value: 'true'
        - name: OLLAMA_KV_CACHE_TYPE
          value: q8_0
      image: 'quay.io/rh-aiservices-bu/ollama-ubi9:0.2.8'
      name: kserve-container
      ports:
        - containerPort: 11434
          name: http1
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: any
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
----


=== Inference Service

Models can be prepopulated - models are stored in Ollama user's directory .ollama/models. 

.PVC for Models
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ollama-data
  namespace: ollama
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: org.democratic-csi.nfs
    volume.kubernetes.io/storage-provisioner: org.democratic-csi.nfs
  finalizers:
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 200Gi
  storageClassName: zfs-generic-nfs-csi
  volumeMode: Filesystem
----

.Generic Ollama Inference Service
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: ollama
    serving.knative.openshift.io/enablePassthrough: 'true'
    serving.kserve.io/deploymentMode: Serverless
    sidecar.istio.io/inject: 'true'
    sidecar.istio.io/rewriteAppHTTPProbers: 'true'
  name: ollama-inference-service
  namespace: ai-project
  finalizers:
    - odh.inferenceservice.finalizers
    - inferenceservice.finalizers
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: any
      name: ''
      resources:
        limits:
          cpu: '8'
          memory: 10Gi
          nvidia.com/gpu: '2'
        requests:
          cpu: '4'
          memory: 8Gi
          nvidia.com/gpu: '2'
      runtime: ollama-serving-runtime
      storage:
        key: model-bucket
        path: ollama
      volumeMounts:
        - mountPath: /.ollama/models/
          name: ollama-data
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
    volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: ollama-data
----
