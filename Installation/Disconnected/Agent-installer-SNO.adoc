
= Agent Installer

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]



== Get all the relevant binaries

.Download binaries
[source,bash]
----
OCP_VERSION=4.16.16
ARCH=x86_64
curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-client-linux.tar.gz -o oc.tar.gz
tar zxf oc.tar.gz
curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-install-linux.tar.gz -o openshift-install-linux.tar.gz
tar zxvf openshift-install-linux.tar.gz
ISO_URL=$(./openshift-install coreos print-stream-json | grep location | grep $ARCH | grep iso | cut -d\" -f4)
curl -L $ISO_URL -o rhcos-live.iso
curl https://mirror.openshift.com/pub/openshift-v4/clients/butane/latest/butane-amd64 -o butane
curl https://mirror.openshift.com/pub/openshift-v4/clients/coreos-installer/latest/coreos-installer_amd64 -o coreos-installer
chmod +x coreos-installer 
chmod +x butane
---- 

== Single Node 

=== Create ISO - DHCP Configuration

.install-config.yaml
[source,yaml]
----
apiVersion: v1
baseDomain: <domain> 
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 1 
metadata:
  name: <name> 
networking: 
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16 
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: /dev/disk/by-id/<disk_id> 
pullSecret: '<pull_secret>' 
sshKey: |
  <ssh_key> 
imageDigestSources:
  - mirrors:
    - mirror: <repostiry>:<repository_port>/<mirror_repo>
    source: quay.io

----

TIP: To determine /dev/disk/by-id/<disk_id>, boot rhcos-live.iso on target machine. Do a 'lsblk -o model,size,path' or 'ls -l /dev/disk/by-path' and detremine drive. (or you can specify /dev/sdX format).

TIP: Be sure to include mirror registry's secret in the pullSecret.

WARNING: Remove cloud.redhat.com from the pull secret to disable insights.


.Create ISO
[source,bash]
----
mkdir sno
cp install-config.yaml sno
./openshift-install --dir=sno create single-node-ignition-config
#alias coreos-installer='podman run --privileged --pull always --rm -v /dev:/dev -v /run/udev:/run/udev -v $PWD:/data -w /data quay.io/coreos/coreos-installer:release'
coreos-installer iso ignition embed -fi sno/bootstrap-in-place-for-live-iso.ign -o sno-install.iso rhcos-live.iso.org
----

=== Create ISO - Static Network

.Install "nmstatectl"
[source,bash]
----
sudo dnf install nmstatectl -y
----

[TIP]
====
If you need to determine any values, for example rootdevice, or network configuration, you can boot rhcos-live.iso on target machine.

.Do a "sudo nmtui" to configure network as required, to capute network configuration.
----
nmtui
nmstatectl show > nmstate.config
----

.lsblk 
----
lsblk -o path,size,model,SERIAL,TYPE
----

.Link back to disk by path
----
ls -l /dev/disk/by-path
----

====

Create install-config.yaml and agent-config.yaml. Use the nmstate.config captured above to help with the agent network configuration.


.install-config.yaml
[source,yaml]
----
apiVersion: v1
baseDomain: <base-domain> 
compute:
- architecture: amd64
  hyperthreading: Enabled
  name: worker
  replicas: 0 
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  replicas: 1 
metadata:
  name: <cluster-name>
networking: 
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 192.168.100.0/24
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: ''
sshKey: |
  ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPHOkTjC/ErGlk/jjjl0kFeT2ypOojwwLWtdcnXcRqAz ai@disconnected.pietersmalan.com
imageDigestSources:
- mirrors:
  - <mirror-registry>/<mirror-repo>/openshift4/openshift/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
- mirrors:
  - <mirror-registry>/<mirror-repo>/openshift4/openshift/release-images
  source: quay.io/openshift-release-dev/ocp-release
additionalTrustBundle: |
    -----BEGIN CERTIFICATE-----
    -----END CERTIFICATE-----

----

TIP: To determine /dev/disk/by-id/<disk_id>, boot rhcos-live.iso on target machine. Do a 'lsblk -o model,size,path' or 'ls -l /dev/disk/by-path' and detremine drive. (or you can specify /dev/sdX format).

TIP: Be sure to include mirror registry's secret in the pullSecret.

WARNING: Remove cloud.redhat.com from the pull secret to disable insights.


.agent-config-yaml
[source,yaml]
----
apiVersion: v1beta1
kind: AgentConfig
metadata:
  name: <cluster-name>
rendezvousIP: 192.168.100.3 
hosts:
  - hostname: <cluster-name>.<base-domain>
    rootDeviceHints: 
      deviceName: /dev/sda
    interfaces:
      - name: enp1s0
        macAddress: 52:54:00:D8:43:65 
    networkConfig:
      interfaces:
        - name: enp1s0
          type: ethernet
          state: up
          mac-address: 52:54:00:D8:43:65
          ipv4:
            enabled: true
            address:
              - ip: 192.168.100.3
                prefix-length: 23 
            dhcp: false
      dns-resolver:
        config:
          server:
          - 192.168.100.249
      routes:
        config:
        - destination: 0.0.0.0/0
          next-hop-address: 192.168.100.249 
          next-hop-interface: enp1s0
          table-id: 254

----

.Create manifests
----
mkdir sno
cp agent-config.yaml install-config.yaml sno
./openshift-install agent create cluster-manifests --dir sno
----

.Create ISO
----
./openshift-install --dir <install_directory> agent create image
----

Wait until server reboots.

== Apply mirror yaml

Last step is to install the mirror generated yamls.

.Connecting to server
----
export KUBECONFIG=sno/auth/kubeconfig
----

.Checking status
----
oc get co -wA
----

WARNING: Wait until all cluster operators are installed. The message column indicates the status.

=== Mirror v1

.Mirror v1 (default oc mirror) Locate the yaml under the mirror directory, for example, mirror/oc-mirror-workspace/results-...
----
oc apply -f imageContentSourcePolicy.yaml
----

Before applying catalogSource-cs-redhat-operator-index.yaml, edit the file and change the name to redhat-operator-index
----
name: redhat-operator-index
----

.Apply the changed yaml
----
oc apply -f catalogSource-cs-redhat-operator-index.yaml
----

=== Mirror v2

.Apply all yaml files in the working-dir/cluster-resources directory as created by the oc mirror v2 command.
----
oc apply -f working-dir/cluster-resources
----

== Disable Market Place sources

.Patch the operator hub
----
oc patch OperatorHub cluster --type json -p '[{"op": "add", "path": "/spec/disableAllDefaultSources", "value": true}]'
----

== Prepare local LVM storage 

.local-storage LVM Operator subscription
[source,yaml]
----
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    openshift.io/cluster-monitoring: "true"
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged
  name: openshift-storage
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-storage-operatorgroup
  namespace: openshift-storage
spec:
  targetNamespaces:
  - openshift-storage
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: lvms
  namespace: openshift-storage
spec:
  installPlanApproval: Automatic
  name: lvms-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----

.Create LVMCluster custom resource to provision
[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: my-lvmcluster
  namespace: openshift-storage
spec:
  storage:
    deviceClasses:
    - default: true
      deviceSelector:
        forceWipeDevicesAndDestroyAllData: true
        paths:
        - /dev/disk/by-path/<as determined by ls -l /dev/disk/by-path>
      fstype: xfs
      name: vg1
      thinPoolConfig:
        chunkSizeCalculationPolicy: Static
        name: thin-pool-1
        overprovisionRatio: 10
        sizePercent: 90

----

== Prepare Virtualization

.Create Openshift Virtualization subscription, and instance
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-cnv
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kubevirt-hyperconverged-group
  namespace: openshift-cnv
spec:
  targetNamespaces:
    - openshift-cnv
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  startingCSV: kubevirt-hyperconverged-operator.v4.16.0
  channel: "stable" 
---
----

Before creating the virtualization instance, we have to set the target storage for the machine template imports.

.Set the new default strage class for virtualization
----
oc patch storageclass lvms-vg1 -p '{"metadata":{"annotations":{"storageclass.kubevirt.io/is-default-virt-class":"true"}}}' 
----

.Create StorageProfile
[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: lvms-vg1
spec:
  claimPropertySets:
    - accessModes:
        - ReadWriteOnce
      volumeMode: Block
    - accessModes:
        - ReadWriteOnce
      volumeMode: Filesystem
  cloneStrategy: csi-clone 
  dataImportCronSourceFormat: pvc
----

.Create image streams
[source,yaml]
----
kind: ImageStream
apiVersion: image.openshift.io/v1
metadata:
  name: rhel8-guest
  namespace: openshift-virtualization-os-images
  labels:
    app: kubevirt-hyperconverged
    app.kubernetes.io/component: compute
    app.kubernetes.io/part-of: hyperconverged-cluster
    app.kubernetes.io/version: 4.16.3
spec:
  lookupPolicy:
    local: false
  tags:
    - name: latest
      annotations: null
      from:
        kind: DockerImage
        name: <mirrorregistry>/<mirror_repo>/rhel8/rhel-guest-image
      generation: 32
      importPolicy:
        scheduled: true
        importMode: Legacy
      referencePolicy:
        type: Source
---
kind: ImageStream
apiVersion: image.openshift.io/v1
metadata:
  name: rhel9-guest
  namespace: openshift-virtualization-os-images
  labels:
    app: kubevirt-hyperconverged
    app.kubernetes.io/component: compute
    app.kubernetes.io/part-of: hyperconverged-cluster
    app.kubernetes.io/version: 4.16.3
spec:
  lookupPolicy:
    local: false
  tags:
    - name: latest
      annotations: null
      from:
        kind: DockerImage
        name: <mirrorregistry>/<mirror_repo>/rhel9/rhel-guest-image
      generation: 41
      importPolicy:
        scheduled: true
        importMode: Legacy
      referencePolicy:
        type: Source
----

.Create the hyperconverged instance
[source,yaml]
----
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  featureGates:
    enableCommonBootImageImport: true
  dataImportCronTemplates:
    - metadata:
        annotations:
          cdi.kubevirt.io/storage.bind.immediate.requested: 'true'
        labels:
          instancetype.kubevirt.io/default-instancetype: u1.medium
          instancetype.kubevirt.io/default-preference: rhel.8
        name: rhel8-image-cron
      spec:
        garbageCollect: Outdated
        managedDataSource: rhel8
        schedule: 7 5/12 * * *
        template:
          metadata: {}
          spec:
            source:
              registry:
                imageStream: rhel8-guest
                pullMethod: node
            storage:
              resources:
                requests:
                  storage: 30Gi
          status: {}
      status:
        commonTemplate: true
    - metadata:
        annotations:
          cdi.kubevirt.io/storage.bind.immediate.requested: 'true'
        labels:
          instancetype.kubevirt.io/default-instancetype: u1.medium
          instancetype.kubevirt.io/default-preference: rhel.9
          kubevirt.io/dynamic-credentials-support: 'true'
        name: rhel9-image-cron
      spec:
        garbageCollect: Outdated
        managedDataSource: rhel9
        schedule: 7 5/12 * * *
        template:
          metadata: {}
          spec:
            source:
              registry:
                imageStream: rhel9-guest
                pullMethod: node
            storage:
              resources:
                requests:
                  storage: 30Gi
          status: {}
      status:
        commonTemplate: true
  dataImportSchedule: 7 5/12 * * *
     
----

.Create an import for required OS images
[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataImportCron
metadata:
  annotations:
    cdi.kubevirt.io/storage.bind.immediate.requested: 'true'
    cdi.kubevirt.io/storage.import.imageStreamDockerRef: '<mirrorregistry>/<mirror>/ocp/rhel9/rhel-guest-image:latest'
    operator-sdk/primary-resource: openshift-cnv/ssp-kubevirt-hyperconverged
    operator-sdk/primary-resource-type: SSP.ssp.kubevirt.io
  name: rhel9-image-cron
  namespace: openshift-virtualization-os-images
  labels:
    app.kubernetes.io/component: templating
    app.kubernetes.io/managed-by: ssp-operator
    app.kubernetes.io/name: data-sources
    app.kubernetes.io/part-of: hyperconverged-cluster
    app.kubernetes.io/version: 4.16.3
    instancetype.kubevirt.io/default-instancetype: u1.medium
    instancetype.kubevirt.io/default-preference: rhel.9
    kubevirt.io/dynamic-credentials-support: 'true'
spec:
  garbageCollect: Outdated
  managedDataSource: rhel9
  schedule: 7 5/12 * * *
  template:
    metadata: {}
    spec:
      source:
        registry:
          imageStream: rhel9-guest
          pullMethod: node
      storage:
        resources:
          requests:
            storage: 30Gi
----

== Prepare Migration Toolkit for Virtualization

=== Prepare VDDK image 

The VDDK speeds up migration from VMware and is highly recommended. The image would be referenced in the configuration of source's host.

TIP: Download relevant VDDK version associated with your VMware version

https://developer.vmware.com/web/sdk/8.0/vddk
https://developer.vmware.com/web/sdk/7.0/vddk

.Build images
[source,bash] 
----
tar zxvf VMware-vix-disklib-<version>.x86_64.tar.gz
cat > Dockerfile << EOF
FROM <mirrorregistry/<mirror_repo>/ubi8/ubi:latest
USER 1001
COPY vmware-vix-disklib-distrib /vmware-vix-disklib-distrib
RUN mkdir -p /opt
ENTRYPOINT ["cp", "-r", "/vmware-vix-disklib-distrib", "/opt"]
EOF
podman build . -t <mirrorregistry/<mirror_repo>/vddk:<tag>
podman push <mirrorregistry/<mirror_repo>/vddk:<tag>
----

== Install MTV Operator

.Create operator
[source,yaml]
----
cat << EOF | oc apply -f -
---
apiVersion: project.openshift.io/v1
kind: Project
metadata:
  name: openshift-mtv
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: migration
  namespace: openshift-mtv
spec:
  targetNamespaces:
    - openshift-mtv
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: mtv-operator
  namespace: openshift-mtv
spec:
  channel: release-v2.7
  installPlanApproval: Automatic
  name: mtv-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: "mtv-operator.v2.7.2"
---
apiVersion: forklift.konveyor.io/v1beta1
kind: ForkliftController
metadata:
  name: forklift-controller
  namespace: openshift-mtv
spec:
  olm_managed: true
  feature_ui_plugin: 'true'
  feature_validation: 'true'
  feature_volume_populator: 'true'
EOF
----

== Setup custom CA and ingress Certificate

.CA
[source,yaml]
----
oc delete configmap custom-ca -n openshift-config
oc create configmap custom-ca --from-file=ca-bundle.crt=fullchain.pem -n openshift-config
oc delete secret tlssecret -n openshift-ingress
oc create secret tls tlssecret --cert=fullchain.pem --key=privkey.pem -n openshift-ingress
oc patch ingresscontroller.operator default --type=merge -p '{"spec":{"defaultCertificate": {"name": "tlssecret"}}}' -n openshift-ingress-operator
oc patch proxy/cluster --type=merge --patch='{"spec":{"trustedCA":{"name":"custom-ca"}}}'
----

== Setup Openshift Update Service

.Update Services Operator
----
cat << EOF | oc apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-update-service
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true" 
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: update-service-operator-group
  namespace: openshift-update-service
spec:
  targetNamespaces:
  - openshift-update-service
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: update-service-subscription
  namespace: openshift-update-service
spec:
  channel: v1
  installPlanApproval: "Automatic"
  source: "redhat-operators" 
  sourceNamespace: "openshift-marketplace"
  name: "cincinnati-operator"
---
EOF
----

.Apply updateService.yaml generated by the mirror command 
[source,bash]
----
oc apply -f ./oc-mirror-workspace/results-..../updateService.yaml
----

.CA Certificate for mirror
----
cat << EOF | oc apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: updateservice-registry-ca
  namespace: openshift-config
data:
  updateservice-registry: | 
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
EOF
----

.Set Update End Poit
[source,bash]
----
POLICY_ENGINE_GRAPH_URI="$(oc -n openshift-update-service get updateservice <update-service-name> -o jsonpath='{.status.policyEngineURI}/api/upgrades_info/v1/graph')"
PATCH="{\"spec\":{\"upstream\":\"${POLICY_ENGINE_GRAPH_URI}\"}}"
$ oc patch clusterversion version -p $PATCH --type merge
----

.Setup CA reference in clusterVersion
[source,bash]
----
oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"updateservice-registry-ca"}}}' --type=merge
----

== Updating Cluster

.Delete Insights Operator
----
oc delete clusteroperator insights
----

Change imagegset.yaml to reflect new version(s), and the mirror the latest images using oc mirror.

NOTE: To see latest releases you can use the following link: https://console.redhat.com/openshift/releases

.To list available updates
[source,bash]
----
oc adm upgrade
----

.Trigger Update, optionally specifying to version. Or use "--to-latest=true" to upgrade to lateset version.
[source,bash]
----
oc adm upgrade --to=<version>
----

.Force the update
[source,bash]
----
oc patch clusterversion version --type json -p '[{"op": "add", "path": "/spec/desiredUpdate/force", "value": true}]'
----

.To monitor upgrade
[source,bash]
----
oc adm upgrade
----

