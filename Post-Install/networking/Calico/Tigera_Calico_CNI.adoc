:toc:
:toc-placement!:

== OVNKubernetes to Tigera Calico migration


toc::[]


ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

These steps were followed to migrate existing OCP 4.17/4.18 cluster from OVNKuberentes CNI to Calico Enterprise v3.20 CNI

=== Pause MachineConfigPool and prepare NetworkOperator to be migrated

[source,bash]
----
oc patch MachineConfigPool master --type='merge' --patch '{ "spec": { "paused": true } }'
oc patch MachineConfigPool worker --type='merge' --patch '{ "spec":{ "paused": true } }'
oc patch Network.operator.openshift.io cluster --type='merge' --patch '{ "spec": { "migration": null } }'
oc patch Network.operator.openshift.io cluster --type='merge' --patch '{ "spec": { "migration": { "networkType": "Calico" } } }'
----

=== Prepare Calico manifests

[source,bash]
----
CALICO_VERSION=v3.20.3
mkdir calico
wget -qO- https://downloads.tigera.io/ee/$CALICO_VERSION/manifests/ocp.tgz | tar xvz --strip-components=1 -C calico
mkdir calico-cr
mv calico/01-cr-* calico-cr/
----

NOTE: For Open Source Calico, use `https://github.com/projectcalico/calico/releases/download/$CALICO_VERSION/ocp.tgz`, where $CALICO_VERSION is `v3.29.3`

==== Setup pull-secret if using upstream Tigera image registry (Only EE)

If you use image registry that requires authentication to pull Tigera images, such as default Tigera's `quay.io/tigera` image registry, then configure pull-secret in the `calico/02-pull-secret.yaml` file.

If you use private image registry, then put all necessary images into the private image registry following [this doc](https://docs.tigera.io/calico-enterprise/latest/getting-started/install-on-clusters/private-registry/).

==== Configure the Installation resource

By default, Calico configures IPPool to use IPIP encapsulation. If you prefer VXLAN, or no encapsulation for pod network in the cluster, configure the `calicoNetwork` section in the `Installation` CR inside the `01-cr-installation.yaml` file.

NOTE: If you prefer to use the free Calico versio, then you can set the variant to `Calico`.

.Example configuration for the default IPPool in VXLAN mode
[source,YAML]
----
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  variant: TigeraSecureEnterprise

  imagePullSecrets:
    - name: tigera-pull-secret

  # Registry to use for pulling Calico Enterprise images.
  # registry: <my-registry>

  calicoNetwork:
    bgp: Disabled
    ipPools:
      - cidr: 10.128.0.0/14
        encapsulation: VXLAN
----

NOTE: If you intend to peer your cluster over BGP with upstream routers or switches, then set `bgp: Enabled` in the `Installation` CR. It also can be done later when you configure BGP peering resources.

=== Deploy tigera-operator and related resources

Deploy namespaces and tigera-operator related resources

[source,bash]
----
oc create -f calico/
----

==== Configure adminnetworkpolicy CRD to ignore unsupported warning

NOTE: This is a workaround for an issue in tigera-operator which should be fixed in f
uture release

[source,bash]
----
oc patch crd adminnetworkpolicies.policy.networking.k8s.io --type=merge -p='{"metadata":{"annotations":{"unsupported.operator.tigera.io/ignore": "true"}}}'
----

==== Configure Calico dataplane

[source,bash]
----
oc patch networks.operator.openshift.io cluster --type merge -p '{"spec":{"deployKubeProxy": true}}'
----

NOTE: If using eBPF dataplane, configure eBPF follow [eBPF doc](https://docs.tigera.io/calico/latest/operations/ebpf/enabling-ebpf) to configure Calico in eBPF mode

==== Apply Calico CRs

Check if Calico CRDs already deployed before applying CRs

[source,bash]
----
oc get crds | grep projectcalico
----

When Calico CRDs are in place, apply Calico CRs

[source,bash]
----
oc create -f calico-cr/
----

Wait for all components to become available

[source,bash]
----
oc get tigerastatus
----

=== Convert OCP networking to Calico

[source,bash]
----
oc patch Network.config.openshift.io cluster --type='merge' --patch '{ "spec": { "networkType": "Calico" } }'
----

==== Restart Multus

[source,bash]
----
oc -n openshift-multus rollout restart daemonset/multus
oc -n openshift-multus -w --timeout=2m rollout status daemonset/multus
----

==== Change Default Network to Calico

[source,bash]
----
oc patch Network.operator.openshift.io cluster --type='merge' --patch '{ "spec": { "migration": null } }'
oc patch Network.operator.openshift.io cluster --type='merge' --patch '{ "spec": { "defaultNetwork": { "ovnKubernetesConfig":null } } }'
# if this command gets stuck reboot nodes
----

==== Remove OVN

[source,bash]
----
oc delete namespace openshift-ovn-kubernetes
----

=== Unpause MachineConfigPools

[source,bash]
----
oc patch MachineConfigPool master --type='merge' --patch '{ "spec": { "paused": false } }'
oc patch MachineConfigPool worker --type='merge' --patch '{ "spec":{ "paused": false } }'
----

=== Apply Calico Enterprise resources (Only EE)

Once Tigera Calico is available, deploy Calico Enterprise license

.Check status of apiserver
[source,bash]
----
oc get tigerastatus apiserver -ojsonpath='{.status.conditions[?(@.type=="Available")].status}'
----

.When ready, apply license
[source,bash]
----
oc apply -f /path/to/license.yaml
----

==== Create StorageClass for ElasticSearch

TIP: Use an existing storage class definition for a file system based storage to create a similar StorageClass as below.

.Sample Storage Class
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: tigera-elasticsearch
  labels:
  annotations:
    description: Tigera ElasticSearch Storage Class for Enterprise Features
provisioner: topolvm.io
parameters:
  csi.storage.k8s.io/fstype: xfs
  topolvm.io/device-class: vg1
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
----

==== Apply Tiger Enterprise Resources

[source,bash]
----
oc create -f https://downloads.tigera.io/ee/v3.20.3/manifests/ocp/tigera-enterprise-resources.yaml
#oc create -f https://downloads.tigera.io/ee/v3.20.3/manifests/ocp/tigera-prometheus-operator.yaml
----

==== Create a route to Tigera Manager

[source,bash]
----
cat <<EOF> route.yaml
route.yaml
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: tigera-manager
  namespace: tigera-manager
spec:
  host: manager
  to:
    kind: Service
    name: tigera-manager
    weight: 100
  port:
    targetPort: 9443
  tls:
    termination: passthrough
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
EOF

oc apply -f route.yaml
----

==== Create Service Account

[source,bash]
----
oc create sa calico-admin -n default
oc create clusterrolebinding calico-admin-access --clusterrole tigera-network-admin --serviceaccount default:calico-admin
oc create token calico-admin -n default--duration=24h -ojsonpath='{.status.token}'
----


== Sample Network Attachment for Reference

.nad.yaml
[source,yaml]
----
apiVersion: 'k8s.cni.cncf.io/v1'
kind: NetworkAttachmentDefinition
metadata:
  name: additional-calico-network
spec:
  config: '{
    "cniVersion": "0.3.1",
    "type": "calico",
    "log_level": "info",
    "datastore_type": "kubernetes",
    "mtu": 1410,
    "nodename_file_optional": false,
    "ipam": {
      "type": "calico-ipam",
      "assign_ipv4": "true",
      "assign_ipv6": "false"
    },
    "policy": {
      "type": "k8s"
    },
    "kubernetes": {
      "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
    }
  }'
----
